{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deezer playlist dataset and music recommendation using word2vec\n",
    "\n",
    "In this mini project we will develop a word2vec network and use it to build a playlist completion tool (song suggestion). The data is hosted on the following repository: \n",
    "http://github.com/comeetie/deezerplay.git. \n",
    "To learn more about word2vec and the data we are going to use you can read the two following references:\n",
    "\n",
    "- Efficient estimation of word representations in vector space, Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. (https://arxiv.org/abs/1301.3781)\n",
    "- Word2vec applied to Recommendation: Hyperparameters Matter, H. Caselles-Dupré, F. Lesaint and J. Royo-Letelier. (https://arxiv.org/pdf/1804.04212.pdf)\n",
    "\n",
    "This project was supervised by Mr. [Etienne Côme, @comeetie](http://github.com/comeetie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "The data is in the form of a playlist. Each playlist is a list with the Deezer ID of the piece followed by the artist ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import de Keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Dense,Flatten\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from sklearn.neighbors import KDTree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100000, 24.21338]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(\"./data/music_2.npy\",allow_pickle=True)\n",
    "[len(data), np.mean([len(p) for p in data])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are going to work on contains 100000 playlists which are composed of an average of 24.1 songs. We will start by keeping only the song identifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get artist and track ID in two different lists\n",
    "playlist_track = [list(filter(lambda w: w.split(\"_\")[0]==u\"track\",playlist)) for playlist in data]\n",
    "playlist_artist = [list(filter(lambda w: w.split(\"_\")[0]==u\"artist\",playlist)) for playlist in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338509"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks = np.unique(np.concatenate(playlist_track))\n",
    "Vt = len(tracks)\n",
    "Vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of different pieces in this data-set is quite high with more than 300,000 pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set of trakcs\n",
    "We will assign to each son an integer that will serve as a unique identifier and input for our network. In order to save a little bit of resources we will only work in this project on songs that appear in at least two playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_counts = dict((tracks[i],0) for i in range(0, Vt))\n",
    "for p in playlist_track:\n",
    "    for a in p:\n",
    "        track_counts[a]=track_counts[a]+1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infrequent piece filtering to save a little bit of time in view of our computing time resources  \n",
    "playlist_track_filter = [list(filter(lambda a : track_counts[a]> 1, playlist)) for playlist in playlist_track]\n",
    "# Get the count\n",
    "counts  =  np.array(list(track_counts.values()))\n",
    "# Sort\n",
    "order = np.argsort(-counts)\n",
    "# creation of our Deezer ID list\n",
    "tracks_list_ordered = np.array(list(track_counts.keys()))[order]\n",
    "# Size of our set = number of pieces preserved\n",
    "Vt=np.where(counts[order]==1)[0][0]\n",
    "\n",
    "track_dict = dict((tracks_list_ordered[i],i) for i in range(0, Vt))\n",
    "# conversion of playlist to integer list\n",
    "corpus_num_track = [[track_dict[track] for track in play ] for play in playlist_track_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and validation set\n",
    "\n",
    "To learn the parameters of our method we will keep the first $l-1$ songs of each playlist (with $l$ the length of the playlist) for learning. To evaluate the completion performance of our method we keep for each playlist the last two songs. The objective will be to find the last one from the second last one. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and valisation indexes\n",
    "index_tst = np.random.choice(100000,20000)\n",
    "index_val = np.setdiff1d(range(100000),index_tst)\n",
    "# The beginning of each playlist is kept for learning purposes\n",
    "play_app  = [corpus_num_track[i][:(len(corpus_num_track[i])-1)] \n",
    "             for i in range(len(corpus_num_track)) if len(corpus_num_track[i])>1]\n",
    "# The last two elements for testing and validation\n",
    "play_tst  = np.array([corpus_num_track[i][(len(corpus_num_track[i])-2):len(corpus_num_track[i])] \n",
    "             for i in index_tst if len(corpus_num_track[i])>3])\n",
    "play_val  = np.array([corpus_num_track[i][(len(corpus_num_track[i])-2):len(corpus_num_track[i])] \n",
    "             for i in index_val if len(corpus_num_track[i])>3])[:10000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyper-parameters of word2vec :\n",
    "\n",
    "The word2vec method involves a number of hyper parameters. We are going to define them and give them first values that we will refine later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of the latent space\n",
    "vector_dim = 30\n",
    "# Window width (see references)\n",
    "window_width = 3\n",
    "# Over-sampling of negative examples\n",
    "neg_sample = 5\n",
    "# Mini-batchs size\n",
    "min_batch_size = 50\n",
    "\n",
    "samp_coef = 0.5\n",
    "#  Subsampling coeff\n",
    "sub_samp = 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création des tables de probabilité de tirage (lissée) et non lissée\n",
    "\n",
    "Pour tirer les exmples négatif nous avons besoin des fréquence lissé de chaque morceau dans notre dataset. De même pour sous échantilloner les morceaux très fréquents nous avons besoin des fréquence brutes. Nous allons calculer ces deux vecteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recupération des comptage\n",
    "counts = np.array(list(track_counts.values()),dtype='float')[order[:Vt]]\n",
    "# normalisation\n",
    "st =  counts/np.sum(counts)\n",
    "# lissage\n",
    "st_smooth = np.power(st,samp_coef)\n",
    "st_smooth = st_smooth/np.sum(st_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_target = Input((1,), dtype='int32')\n",
    "input_context = Input((1,), dtype='int32')\n",
    "\n",
    "emb = Embedding(input_dim = Vt, output_dim = vector_dim)\n",
    "Z1 = emb(input_target)\n",
    "Z2 = emb(input_context)\n",
    "dot_product = Dot(axes = 2, name = 'dot')([Z1, Z2])\n",
    "flat = Flatten(name= 'flatten')(dot_product)\n",
    "output = Dense(1, activation='sigmoid',name=\"classif\")(flat)\n",
    "\n",
    "# Model def\n",
    "Track2Vec = Model(inputs=[input_target, input_context], outputs=output)\n",
    "Track2Vec.compile(loss='binary_crossentropy', optimizer='adam',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 30)        3697230     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 1)         0           embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1)            0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "classif (Dense)                 (None, 1)            2           flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,697,232\n",
      "Trainable params: 3,697,232\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Track2Vec.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gen\n",
    "\n",
    "To learn the projection layer at the heart of our model we will build a generator of positive and negative pair examples of close or random pieces from our training data. The following function will allow us to generate such examples from a playlist (seq) provided as input. This function will first build all the pairs of songs that can be extracted from the sequence if they are within (windows) distance of each other. These pairs will constitute the positive pairs. The pairs concerning two very frequent songs will be removed with a probability that depends on their frequencies. Finally a number of negative examples (corresponding to neg_samples * positive number of examples) will be randomly drawn using the neg_sampling_table. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function generating the data associated with a sequence\n",
    "# seq: input sequence\n",
    "# neg_samples: number of negative examples generated for example positive\n",
    "# neg_sampling_table: probability of negative sample drawing\n",
    "# sub sampling_table: probability used to sub-sample\n",
    "# sub_t: sub-sampling parameter\n",
    "\n",
    "def word2vecSampling(seq,window,neg_samples,neg_sampling_table,sub_sampling_table,sub_t):\n",
    "    # taille du vocabulaire\n",
    "    V = len(neg_sampling_table)\n",
    "    # créations des paires positives a partir de la séquence\n",
    "    positives = skipgrams(sequence=seq, vocabulary_size=V, window_size=window,negative_samples=0)\n",
    "    ppairs    = np.array(positives[0])\n",
    "    # sous échantillonage\n",
    "    if (ppairs.shape[0]>0):\n",
    "        f = sub_sampling_table[ppairs[:,0]]\n",
    "        subprob = ((f-sub_t)/f)-np.sqrt(sub_t/f)\n",
    "        tokeep = (subprob<np.random.uniform(size=subprob.shape[0])) | (subprob<0)\n",
    "        ppairs = ppairs[tokeep,:]\n",
    "    nbneg     = ppairs.shape[0]*neg_samples\n",
    "    # tirage des paires négatives\n",
    "    if (nbneg > 0):\n",
    "        negex     = np.random.choice(V, nbneg, p=neg_sampling_table)\n",
    "        negexcontext = np.repeat(ppairs[:,0],neg_samples)\n",
    "        npairs    = np.transpose(np.stack([negexcontext,negex]))\n",
    "        pairs     = np.concatenate([ppairs,npairs],axis=0)\n",
    "        labels    = np.concatenate([np.repeat(1,ppairs.shape[0]),np.repeat(0,nbneg)])\n",
    "        perm      = np.random.permutation(len(labels))\n",
    "        res = [pairs[perm,:],labels[perm]]\n",
    "    else:\n",
    "        res=[[],[]]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive and negative examples generation\n",
    "*track_ns_generator* will generate positive and negative examples from randomly drawn \"nbm\" playlists in the \"corpus_num\" dataset provided as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def track_ns_generator(corpus_num,nbm):\n",
    "    \n",
    "    while 1:\n",
    "        \n",
    "        c1 = np.array([])\n",
    "        c2 = np.array([])\n",
    "        y = np.array([])\n",
    "        sample_cor = random.sample(corpus_num, nbm)\n",
    "        for playlist in sample_cor:\n",
    "            obj = word2vecSampling(seq= playlist, window=window_width, neg_samples=neg_sample, neg_sampling_table=st_smooth,\n",
    "                            sub_sampling_table= st, sub_t = sub_samp)\n",
    "         \n",
    "            y = np.concatenate((y, obj[1]), axis = 0).astype(int)\n",
    "            dataf = pd.DataFrame(obj[0], columns = ['a','b'])\n",
    "            pp = np.array(dataf.a)\n",
    "            qq = np.array(dataf.b)\n",
    "            c1 = np.concatenate((c1, pp),axis = 0).astype(int)\n",
    "            c2 = np.concatenate((c2, qq),axis = 0).astype(int)\n",
    "        yield ([c1,c2],[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([array([ 1224, 36278, 10712, ..., 12039, 12039, 12039]), array([21657,  1224,  1130, ..., 11746,  3043,  4201])], [array([0, 1, 0, ..., 1, 0, 0])])\n"
     ]
    }
   ],
   "source": [
    "for elt in track_ns_generator(play_app,min_batch_size):\n",
    "    print(elt)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1224., 36278., 10712., ..., 12039., 12039., 12039.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elt[0][0].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-16dc659765d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrack2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_ns_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplay_app\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "hist=Track2Vec.fit(track_ns_generator(play_app,min_batch_size),steps_per_epoch = 200,epochs=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup of latent space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_tracks = Track2Vec.get_weights()[0]\n",
    "with open('latent_positions.npy', 'wb') as f:\n",
    "    np.save(f, vectors_tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use in completion and evaluation\n",
    "We can now use this space to make suggestions.\n",
    "\n",
    "*predict_batch* takes as input a vector of number of pieces (seeds), (s) a number of proposals to make, the vectors of the pieces in the latent space X and a kd-tree allowing to accelerate the calculations of nearest neighbors. To make its propisitions this function will return the indices of the s closest neighbors of each seed. To make these predictions not take too much time we will use a kd-tree (available in scikit learn) to speed up the search for nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdt = KDTree(vectors_tracks, leaf_size=10, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(seeds,k,X,kdt):\n",
    "    # TODO\n",
    "    seeds_neig = pd.DataFrame({})\n",
    "    for elt in seeds:\n",
    "        dim = X[elt].reshape(1, -1)\n",
    "        neigb =  kdt.query(dim, k = k)\n",
    "        seeds_neig[elt] = np.array(neigb[1].reshape(1, -1)[0])\n",
    "    \n",
    "    return seeds_neig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = predict_batch(play_val[:,0],10,vectors_tracks,kdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these suggestions with the second column of play_val (the songs actually present). To do this you will calculate the hit@10 which is 1 if the song actually present in the playlist is one of the 10 suggestions (this score is averaged over the validation set) and the NDCG@10 (Normalized Discounted Cumulative Gain) which takes into account the order of the suggestions. This second score is worth $1/log2(k+1)$ if proposal k (k between 1 and 10) is the correct proposal and 0 if no proposal is correct. As before you will calculate the average score on the validation set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HitatK(tracks_true, tracks_predict) :\n",
    "    score = 0\n",
    "    for i in range(len(tracks_true)) :\n",
    "        if tracks_true[i] in tracks_predict[i] :\n",
    "            score += 1\n",
    "    return(score/len(tracks_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDGCatK(tracks_true, tracks_predict) :\n",
    "    score = 0\n",
    "    for i in range(len(tracks_true)) :\n",
    "        if tracks_true[i] in tracks_predict[i] :\n",
    "            score += 1/np.log2(np.where(tracks_predict[i] == tracks_true[i])[0][0] + 2)\n",
    "    return(score/len(tracks_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDGCatK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HitatK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper parameters preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmb_list = [50, 100]\n",
    "\n",
    "window_list = [2, 3, 5]\n",
    "\n",
    "neg_samples_list = [5, 10]\n",
    "\n",
    "sub_t_list = [0.001, 0.0001, 0.00001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_smooth_list = []\n",
    "samp_coef_list = [0.25, 0.5, 0.75]\n",
    "for alpha in samp_coef_list :\n",
    "    \n",
    "    st_smooth = np.power(st, alpha)\n",
    "    st_smooth = st_smooth/np.sum(st_smooth)\n",
    "    st_smooth_list.append(list(st_smooth))\n",
    "    \n",
    "neg_sampling_table_list = st_smooth_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redefinition of the track_ns_generator function with  hyper parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_ns_generator_GridSearch(corpus_num, nbm, window, neg_samples, neg_sampling_table, sub_t):\n",
    "    while 1 :\n",
    "        corpus_reduced = random.sample(corpus_num, nbm)\n",
    "        x1=[]\n",
    "        x2=[]\n",
    "        y=[]\n",
    "        k=0\n",
    "        for seq in corpus_reduced :\n",
    "            w2v = word2vecSampling(seq, window, neg_samples, neg_sampling_table, sub_sampling_table, sub_t)\n",
    "            for i in range(len(w2v[0])):\n",
    "                x1.append(w2v[0][i][0])\n",
    "                x2.append(w2v[0][i][1])\n",
    "                y.append(w2v[1][i])\n",
    "                k+=1\n",
    "        x1=np.array(x1).reshape(k,)\n",
    "        x2=np.array(x2).reshape(k,)\n",
    "        y=np.array(y).reshape(k,)\n",
    "        \n",
    "        yield ([x1,x2], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training and display of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scores_data = pd.DataFrame()\n",
    "Scores_data[0] = [i for i in range(7)]\n",
    "for i in range(10) :    \n",
    "    nbm = random.sample(nmb_list, 1)[0]\n",
    "    sub_t = random.sample(sub_t_list, 1)[0]\n",
    "    neg_sampling_table = random.sample(neg_sampling_table_list, 1)[0]\n",
    "    neg_samples = random.sample(neg_samples_list, 1)[0]\n",
    "    window = random.sample(window_list, 1)[0]\n",
    "    \n",
    "    Track2Vec = Model(inputs=[input_target, input_context], outputs=output)\n",
    "    Track2Vec.compile(loss='binary_crossentropy', optimizer='adam',metrics=[\"accuracy\"])\n",
    "    Track2Vec.fit(track_ns_generator_GridSearch(play_app, nbm, window, neg_samples, neg_sampling_table,\n",
    "                                                sub_t), steps_per_epoch = 20, epochs = 5)\n",
    "    vectors_tracks = Track2Vec.get_weights()[0]\n",
    "    kdt = KDTree(vectors_tracks, leaf_size = 10, metric = 'euclidean')\n",
    "    indexes = predict_batch(play_tst[:,0], 10, vectors_tracks, kdt)\n",
    "    Scores_data[i+1] = [nbm, sub_t, samp_coef_list[neg_sampling_table_list.index(neg_sampling_table)],\n",
    "                       neg_samples, window, HitatK(play_tst[:,1], indexes), NDGCatK(play_tst[:,1], indexes)]\n",
    "\n",
    "Scores_data = Scores_data.drop(0, axis = 1)\n",
    "Scores_data = Scores_data.T\n",
    "Scores_data.columns = [\"nbm\", \"sub_t\", \"samp_coef\", \"neg_samples\", \"window\", \"HitatK\", \"NDGCatK\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More music\n",
    "\n",
    "The TrackArtists file contains meta.data on the tracks and the artists for a subset of the 300000 tracks present in the dataset. We can use it to search for the number of a song from its title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tr_meta=pd.read_csv(\"./TracksArtists.csv\")\n",
    "joindf = pd.DataFrame({\"track_id\":tracks_list_ordered[:Vt],\"index\":range(Vt)})\n",
    "meta = tr_meta.merge(joindf, left_on=\"track_id\",right_on=\"track_id\")\n",
    "meta.set_index(\"index\",inplace=True)\n",
    "meta[[\"title\",\"name\",\"preview\",\"track_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_track(title):\n",
    "    return meta.loc[meta[\"title\"]==title,:].index[0]\n",
    "\n",
    "tr=find_track(\"Hexagone\")\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radio\n",
    "\n",
    "The Deeezer API allows you to retrieve information about the pieces of the dataset from their Deezer id. Among this information when it is available a url to listen to a free sample is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json \n",
    "def gettrackinfo(number):\n",
    "    track_url =  \"https://api.deezer.com/track/{}\".format(tracks_list_ordered[number].split(\"_\")[1])\n",
    "    with urllib.request.urlopen(track_url) as url:\n",
    "        data = json.loads(url.read().decode())\n",
    "    return data\n",
    "track_apidata = gettrackinfo(find_track(\"Hexagone\"))\n",
    "track_apidata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can use it to listen to an excerpt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Audio, clear_output\n",
    "display(Audio(track_apidata[\"preview\"],autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*start_radio* takes in input a track number in the dataset and launches a series of nb_track tracks by randomly drawing in the neighborhood of the current track the next track to listen to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_radio(seed, nb_candidates, duration, nbsteps = 20):\n",
    "    track_apidata = gettrackinfo(seed)\n",
    "    print(track_apidata[\"title\"])\n",
    "    display(Audio(track_apidata[\"preview\"], autoplay = True))\n",
    "    time.sleep(duration)\n",
    "    clear_output()\n",
    "    track = seed\n",
    "    already_played = [track]    \n",
    "    for i in range(nbsteps):\n",
    "        try :            \n",
    "            next_track = random.choice(list(predict_batch([track], nb_candidates, vectors_tracks, kdt)[0]))\n",
    "            stop = 0\n",
    "            while (next_track in already_played) & (stop < 20) :\n",
    "                next_track = random.choice(list(predict_batch([track], nb_candidates, vectors_tracks, kdt)[0]))\n",
    "                stop += 1\n",
    "            track = next_track\n",
    "            track_apidata = gettrackinfo(track)\n",
    "            print(track_apidata[\"title\"])\n",
    "            display(Audio(track_apidata[\"preview\"], autoplay = True))\n",
    "            time.sleep(duration)\n",
    "            already_played.append(track)\n",
    "        except :\n",
    "            print(\"track not found\")\n",
    "            pass\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_radio(find_track(\"Hexagone\"),5,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
